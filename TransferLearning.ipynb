{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.image as mpimg\n",
    "import os\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from glob import glob\n",
    "import cv2\n",
    "#import zipfile\n",
    "from PIL import Image\n",
    "  \n",
    "from keras.callbacks import EarlyStopping\n",
    "#from keras.preprocessing import image_dataset_from_directory\n",
    "#from keras.preprocessing.image import ImageDataGenerator, load_img\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from keras.utils import image_dataset_from_directory\n",
    "from keras.optimizers import Adam\n",
    "from keras.layers import Conv2D, MaxPooling2D, MaxPool2D\n",
    "from keras.layers import Activation, Dropout, Flatten, Dense\n",
    "from keras.models import Sequential\n",
    "from keras import layers\n",
    "from tensorflow import keras\n",
    "#from layers.experimental.preprocessing import Rescaling\n",
    "from sklearn.model_selection import train_test_split\n",
    "#from zipfile import ZipFile\n",
    "  \n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "43"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#accesing path for dataset\n",
    "data_path='c:\\\\ZHAW\\\\23FS\\\\KI\\\\DataForProject'\n",
    "train_path='c:\\\\ZHAW\\\\23FS\\\\KI\\\\DataForProject\\\\Train'\n",
    "\n",
    "#Variables for resizing\n",
    "IMG_HEIGHT = 30\n",
    "IMG_WIDTH = 30\n",
    "channels = 3\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "#Getting Total Nr of Classes from Train Folder\n",
    "NUM_CATEGORIES = len(os.listdir(train_path))\n",
    "NUM_CATEGORIES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Label Overview\n",
    "classes = { 0:'Speed limit (20km/h)',\n",
    "            1:'Speed limit (30km/h)', \n",
    "            2:'Speed limit (50km/h)', \n",
    "            3:'Speed limit (60km/h)', \n",
    "            4:'Speed limit (70km/h)', \n",
    "            5:'Speed limit (80km/h)', \n",
    "            6:'End of speed limit (80km/h)', \n",
    "            7:'Speed limit (100km/h)', \n",
    "            8:'Speed limit (120km/h)', \n",
    "            9:'No passing', \n",
    "            10:'No passing veh over 3.5 tons', \n",
    "            11:'Right-of-way at intersection', \n",
    "            12:'Priority road', \n",
    "            13:'Yield', \n",
    "            14:'Stop', \n",
    "            15:'No vehicles', \n",
    "            16:'Veh > 3.5 tons prohibited', \n",
    "            17:'No entry', \n",
    "            18:'General caution', \n",
    "            19:'Dangerous curve left', \n",
    "            20:'Dangerous curve right', \n",
    "            21:'Double curve', \n",
    "            22:'Bumpy road', \n",
    "            23:'Slippery road', \n",
    "            24:'Road narrows on the right', \n",
    "            25:'Road work', \n",
    "            26:'Traffic signals', \n",
    "            27:'Pedestrians', \n",
    "            28:'Children crossing', \n",
    "            29:'Bicycles crossing', \n",
    "            30:'Beware of ice/snow',\n",
    "            31:'Wild animals crossing', \n",
    "            32:'End speed + passing limits', \n",
    "            33:'Turn right ahead', \n",
    "            34:'Turn left ahead', \n",
    "            35:'Ahead only', \n",
    "            36:'Go straight or right', \n",
    "            37:'Go straight or left', \n",
    "            38:'Keep right', \n",
    "            39:'Keep left', \n",
    "            40:'Roundabout mandatory', \n",
    "            41:'End of no passing', \n",
    "            42:'End no passing veh > 3.5 tons' }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(39209, 30, 30, 3) (39209,)\n"
     ]
    }
   ],
   "source": [
    "image_data = []\n",
    "image_labels = []\n",
    "\n",
    "for i in range(NUM_CATEGORIES):\n",
    "    path = train_path + \"\\\\\" + str(i)\n",
    "    images = os.listdir(path)\n",
    "\n",
    "    for img in images:\n",
    "        try:\n",
    "            image = cv2.imread(path + '\\\\' + img)\n",
    "            image_fromarray = Image.fromarray(image, 'RGB')\n",
    "            resize_image = image_fromarray.resize((IMG_HEIGHT, IMG_WIDTH))\n",
    "            image_data.append(np.array(resize_image))\n",
    "            image_labels.append(i)\n",
    "        except:\n",
    "            print(\"Error in \" + img)\n",
    "\n",
    "# Changing the list to numpy array\n",
    "image_data = np.array(image_data)\n",
    "image_labels = np.array(image_labels)\n",
    "\n",
    "print(image_data.shape, image_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_valid, y_train, y_valid = train_test_split(image_data, image_labels, test_size=0.3, random_state=42, shuffle=True)\n",
    "\n",
    "X_test = X_train\n",
    "y_test = y_train\n",
    "\n",
    "# rescalling\n",
    "X_train = X_train.astype(np.float32) / 255\n",
    "X_valid = X_valid.astype(np.float32) / 255\n",
    "X_test = X_test.astype(np.float32) / 255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train.shape (27446, 30, 30, 3)\n",
      "X_valid.shape (11763, 30, 30, 3)\n",
      "y_train.shape (27446,)\n",
      "y_valid.shape (11763,)\n",
      "X_test.shape (27446, 30, 30, 3)\n",
      "y_test.shape (27446,)\n",
      "X_train Type: float32\n",
      "X_valid Type float32\n"
     ]
    }
   ],
   "source": [
    "print(\"X_train.shape\", X_train.shape)\n",
    "print(\"X_valid.shape\", X_valid.shape)\n",
    "print(\"y_train.shape\", y_train.shape)\n",
    "print(\"y_valid.shape\", y_valid.shape)\n",
    "print(\"X_test.shape\", X_test.shape)\n",
    "print(\"y_test.shape\", y_test.shape)\n",
    "\n",
    "print(\"X_train Type:\", X_train.dtype)\n",
    "print(\"X_valid Type\", X_valid.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-1.,  1.], dtype=float32)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocess_input = tf.keras.applications.resnet_v2.preprocess_input\n",
    "\n",
    "# example\n",
    "preprocess_input(np.asarray([0, 255]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2,), dtype=float32, numpy=array([-1.,  1.], dtype=float32)>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rescale = tf.keras.layers.Rescaling(1./127.5, offset=-1)\n",
    "\n",
    "# example\n",
    "rescale(np.asarray([0, 255]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMG_SHAPE = (30, 30, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Input size must be at least 32x32; Received `input_shape=(30, 30, 3)`",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32mc:\\ZHAW\\23FS\\KI\\Project\\TransferLearning.ipynb Cell 10\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/ZHAW/23FS/KI/Project/TransferLearning.ipynb#X12sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39m# Create the base model from the pre-trained model EfficientNetB4\u001b[39;00m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/ZHAW/23FS/KI/Project/TransferLearning.ipynb#X12sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m model \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39;49mkeras\u001b[39m.\u001b[39;49mapplications\u001b[39m.\u001b[39;49mMobileNetV3Large(input_shape\u001b[39m=\u001b[39;49mIMG_SHAPE,\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/ZHAW/23FS/KI/Project/TransferLearning.ipynb#X12sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m                                                \n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/ZHAW/23FS/KI/Project/TransferLearning.ipynb#X12sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m                                                \u001b[39m# include_top=True :Includes the fully connected layers for predictions \u001b[39;49;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/ZHAW/23FS/KI/Project/TransferLearning.ipynb#X12sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m                                                include_top\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/ZHAW/23FS/KI/Project/TransferLearning.ipynb#X12sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m \n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/ZHAW/23FS/KI/Project/TransferLearning.ipynb#X12sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m                                                \u001b[39m# weights from the imagenet challenge.\u001b[39;49;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/ZHAW/23FS/KI/Project/TransferLearning.ipynb#X12sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m                                                weights\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mimagenet\u001b[39;49m\u001b[39m'\u001b[39;49m)\n",
      "File \u001b[1;32mc:\\Users\\azegl\\anaconda3\\lib\\site-packages\\keras\\applications\\mobilenet_v3.py:531\u001b[0m, in \u001b[0;36mMobileNetV3Large\u001b[1;34m(input_shape, alpha, minimalistic, include_top, weights, input_tensor, classes, pooling, dropout_rate, classifier_activation, include_preprocessing)\u001b[0m\n\u001b[0;32m    526\u001b[0m     x \u001b[39m=\u001b[39m _inverted_res_block(\n\u001b[0;32m    527\u001b[0m         x, \u001b[39m6\u001b[39m, depth(\u001b[39m160\u001b[39m), kernel, \u001b[39m1\u001b[39m, se_ratio, activation, \u001b[39m14\u001b[39m\n\u001b[0;32m    528\u001b[0m     )\n\u001b[0;32m    529\u001b[0m     \u001b[39mreturn\u001b[39;00m x\n\u001b[1;32m--> 531\u001b[0m \u001b[39mreturn\u001b[39;00m MobileNetV3(\n\u001b[0;32m    532\u001b[0m     stack_fn,\n\u001b[0;32m    533\u001b[0m     \u001b[39m1280\u001b[39;49m,\n\u001b[0;32m    534\u001b[0m     input_shape,\n\u001b[0;32m    535\u001b[0m     alpha,\n\u001b[0;32m    536\u001b[0m     \u001b[39m\"\u001b[39;49m\u001b[39mlarge\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[0;32m    537\u001b[0m     minimalistic,\n\u001b[0;32m    538\u001b[0m     include_top,\n\u001b[0;32m    539\u001b[0m     weights,\n\u001b[0;32m    540\u001b[0m     input_tensor,\n\u001b[0;32m    541\u001b[0m     classes,\n\u001b[0;32m    542\u001b[0m     pooling,\n\u001b[0;32m    543\u001b[0m     dropout_rate,\n\u001b[0;32m    544\u001b[0m     classifier_activation,\n\u001b[0;32m    545\u001b[0m     include_preprocessing,\n\u001b[0;32m    546\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\azegl\\anaconda3\\lib\\site-packages\\keras\\applications\\mobilenet_v3.py:282\u001b[0m, in \u001b[0;36mMobileNetV3\u001b[1;34m(stack_fn, last_point_ch, input_shape, alpha, model_type, minimalistic, include_top, weights, input_tensor, classes, pooling, dropout_rate, classifier_activation, include_preprocessing)\u001b[0m\n\u001b[0;32m    280\u001b[0m cols \u001b[39m=\u001b[39m input_shape[col_axis]\n\u001b[0;32m    281\u001b[0m \u001b[39mif\u001b[39;00m rows \u001b[39mand\u001b[39;00m cols \u001b[39mand\u001b[39;00m (rows \u001b[39m<\u001b[39m \u001b[39m32\u001b[39m \u001b[39mor\u001b[39;00m cols \u001b[39m<\u001b[39m \u001b[39m32\u001b[39m):\n\u001b[1;32m--> 282\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m    283\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mInput size must be at least 32x32; Received `input_shape=\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    284\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00minput_shape\u001b[39m}\u001b[39;00m\u001b[39m`\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    285\u001b[0m     )\n\u001b[0;32m    286\u001b[0m \u001b[39mif\u001b[39;00m weights \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mimagenet\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[0;32m    287\u001b[0m     \u001b[39mif\u001b[39;00m (\n\u001b[0;32m    288\u001b[0m         \u001b[39mnot\u001b[39;00m minimalistic\n\u001b[0;32m    289\u001b[0m         \u001b[39mand\u001b[39;00m alpha \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m [\u001b[39m0.75\u001b[39m, \u001b[39m1.0\u001b[39m]\n\u001b[0;32m    290\u001b[0m         \u001b[39mor\u001b[39;00m minimalistic\n\u001b[0;32m    291\u001b[0m         \u001b[39mand\u001b[39;00m alpha \u001b[39m!=\u001b[39m \u001b[39m1.0\u001b[39m\n\u001b[0;32m    292\u001b[0m     ):\n",
      "\u001b[1;31mValueError\u001b[0m: Input size must be at least 32x32; Received `input_shape=(30, 30, 3)`"
     ]
    }
   ],
   "source": [
    "# Create the base model from the pre-trained model EfficientNetB4\n",
    "model = tf.keras.applications.MobileNetV3Large(input_shape=IMG_SHAPE,\n",
    "                                               \n",
    "                                               # include_top=True :Includes the fully connected layers for predictions \n",
    "                                               include_top=True,\n",
    "\n",
    "                                               # weights from the imagenet challenge.\n",
    "                                               weights='imagenet')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
